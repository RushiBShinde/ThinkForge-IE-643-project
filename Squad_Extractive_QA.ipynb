{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNh+hV9Yaz05QaoXbPqmeDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RushiBShinde/ThinkForge-IE-643-project/blob/main/Squad_Extractive_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAtUUtVhB9Zq",
        "outputId": "537ff3e7-b1fc-4b6f-9e26-424478cb460f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount drive and install dependencies (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install/upgrade required libraries (uncomment if running in a fresh Colab)\n",
        "# Note: transformers and datasets are usually preinstalled in Colab, but these commands ensure recent versions.\n",
        "!pip install -q transformers datasets[\"torch\"] tqdm\n",
        "\n",
        "# Imports\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIXED Cell 2 for QASPER parquet version ---\n",
        "import json\n",
        "\n",
        "def safe_json_parse(x):\n",
        "    \"\"\"Try to parse a JSON string if possible; otherwise return x unchanged.\"\"\"\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            return json.loads(x)\n",
        "        except Exception:\n",
        "            return x\n",
        "    return x\n",
        "\n",
        "qa_items = []\n",
        "total_questions = 0\n",
        "extractive_questions = 0\n",
        "\n",
        "for ex in ds:\n",
        "    # Parse qas if it’s a JSON string\n",
        "    qas = safe_json_parse(ex.get(\"qas\", []))\n",
        "    if not isinstance(qas, list):\n",
        "        continue\n",
        "\n",
        "    total_questions += len(qas)\n",
        "    context = concat_full_text(safe_json_parse(ex.get(\"full_text\", [])))\n",
        "\n",
        "    for qa in qas:\n",
        "        qa = safe_json_parse(qa)\n",
        "        if not isinstance(qa, dict):\n",
        "            continue\n",
        "\n",
        "        extractive_spans = None\n",
        "\n",
        "        # Try all known answer field variants\n",
        "        if \"extractive_spans\" in qa and qa[\"extractive_spans\"] is not None:\n",
        "            extractive_spans = safe_json_parse(qa[\"extractive_spans\"])\n",
        "        elif \"extractive_answers\" in qa and qa[\"extractive_answers\"] is not None:\n",
        "            extractive_spans = safe_json_parse(qa[\"extractive_answers\"])\n",
        "        elif \"answers\" in qa and qa[\"answers\"] is not None:\n",
        "            answers_field = safe_json_parse(qa[\"answers\"])\n",
        "            cand = []\n",
        "            if isinstance(answers_field, list):\n",
        "                for a in answers_field:\n",
        "                    a = safe_json_parse(a)\n",
        "                    if isinstance(a, str):\n",
        "                        cand.append(a)\n",
        "                    elif isinstance(a, dict):\n",
        "                        a_type = a.get(\"type\") or a.get(\"answer_type\")\n",
        "                        if a_type and \"extract\" in a_type.lower():\n",
        "                            text = a.get(\"text\") or a.get(\"extractive_text\") or a.get(\"extractive_span\")\n",
        "                            if text:\n",
        "                                cand.append(text)\n",
        "                        else:\n",
        "                            text = a.get(\"text\") or a.get(\"answer_text\")\n",
        "                            if text:\n",
        "                                cand.append(text)\n",
        "            if cand:\n",
        "                extractive_spans = cand\n",
        "\n",
        "        # Keep only extractive ones\n",
        "        if extractive_spans and isinstance(extractive_spans, list) and len(extractive_spans) > 0:\n",
        "            first_span = extractive_spans[0]\n",
        "            if isinstance(first_span, dict):\n",
        "                gt = first_span.get(\"text\") or first_span.get(\"span\") or \"\"\n",
        "            else:\n",
        "                gt = str(first_span)\n",
        "            if gt.strip().lower() in {\"yes\", \"no\", \"unanswerable\", \"cannot answer\", \"\"}:\n",
        "                continue\n",
        "            qa_items.append({\n",
        "                \"query\": qa.get(\"question\") or qa.get(\"query\") or \"\",\n",
        "                \"context\": context,\n",
        "                \"gt\": gt\n",
        "            })\n",
        "            extractive_questions += 1\n",
        "\n",
        "print(f\"Loaded {len(ds)} examples (papers).\")\n",
        "print(f\"Total QAs present in those examples (rough): {total_questions}\")\n",
        "print(f\"Extractive QA pairs collected (to evaluate): {len(qa_items)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76k-Rwn_MN5r",
        "outputId": "646978ce-5c3a-49d0-bdd3-eee822fdf96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 50 examples (papers).\n",
            "Total QAs present in those examples (rough): 0\n",
            "Extractive QA pairs collected (to evaluate): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extractive_qas = []\n",
        "\n",
        "for ex in ds:\n",
        "    qas = ex.get(\"qas\")\n",
        "    if not qas:\n",
        "        continue\n",
        "\n",
        "    # qas is likely a dict of {question_id: qa_object}\n",
        "    for qa in qas.values():\n",
        "        if not isinstance(qa, dict):\n",
        "            continue\n",
        "\n",
        "        question = qa.get(\"question\", \"\").strip()\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        answers = qa.get(\"answers\", [])\n",
        "        for ans_set in answers:\n",
        "            if not isinstance(ans_set, dict):\n",
        "                continue\n",
        "\n",
        "            detailed_answers = ans_set.get(\"answer\", [])\n",
        "            for da in detailed_answers:\n",
        "                if not isinstance(da, dict):\n",
        "                    continue\n",
        "\n",
        "                if da.get(\"extractive_spans\"):\n",
        "                    for span in da[\"extractive_spans\"]:\n",
        "                        extractive_qas.append({\n",
        "                            \"paper_id\": ex.get(\"id\", \"\"),\n",
        "                            \"question\": question,\n",
        "                            \"answer\": span\n",
        "                        })\n",
        "                elif da.get(\"free_form_answer\"):\n",
        "                    extractive_qas.append({\n",
        "                        \"paper_id\": ex.get(\"id\", \"\"),\n",
        "                        \"question\": question,\n",
        "                        \"answer\": da[\"free_form_answer\"]\n",
        "                    })\n",
        "\n",
        "print(f\"✅ Extractive QA pairs collected: {len(extractive_qas)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8-r6lyvNz10",
        "outputId": "315ef657-ccbc-48a4-d839-76acdcd5a02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extractive QA pairs collected: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Fixed version for the parquet-structured QASPER dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "def concat_full_text(full_text):\n",
        "    \"\"\"Concatenate all paragraphs into a single string.\"\"\"\n",
        "    try:\n",
        "        if isinstance(full_text, dict) and \"paragraphs\" in full_text:\n",
        "            return \" \".join([\" \".join(p) for p in full_text[\"paragraphs\"]])\n",
        "        elif isinstance(full_text, list):\n",
        "            return \" \".join([\" \".join(p) if isinstance(p, list) else str(p) for p in full_text])\n",
        "        else:\n",
        "            return str(full_text)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "examples = []\n",
        "for ex in tqdm(ds, desc=\"Processing QASPER samples\"):\n",
        "    context = concat_full_text(ex.get(\"full_text\", {}))\n",
        "    qas = ex.get(\"qas\", {})\n",
        "\n",
        "    # Ensure it's a dict (as in your printed structure)\n",
        "    if not isinstance(qas, dict):\n",
        "        continue\n",
        "\n",
        "    questions = qas.get(\"question\", [])\n",
        "    answers_list = qas.get(\"answers\", [])\n",
        "\n",
        "    for i, q in enumerate(questions):\n",
        "        # Each i-th question corresponds to the i-th entry in answers_list\n",
        "        if i >= len(answers_list):\n",
        "            continue\n",
        "\n",
        "        ans_entry = answers_list[i]\n",
        "        if not ans_entry or not isinstance(ans_entry, dict):\n",
        "            continue\n",
        "\n",
        "        # Inside ans_entry, there's a key \"answer\" (a list of answers)\n",
        "        detailed_answers = ans_entry.get(\"answer\", [])\n",
        "        for ans in detailed_answers:\n",
        "            if ans.get(\"unanswerable\"):\n",
        "                continue\n",
        "            extractive_spans = ans.get(\"extractive_spans\", [])\n",
        "            if extractive_spans:\n",
        "                examples.append({\n",
        "                    \"question\": q.strip(),\n",
        "                    \"context\": context.strip(),\n",
        "                    \"ground_truth_answer\": extractive_spans[0].strip()\n",
        "                })\n",
        "                break  # take only the first valid extractive span per question\n",
        "\n",
        "print(f\"✅ Extractive QA pairs collected: {len(examples)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_Ly7pz1QUVm",
        "outputId": "9cdd54c7-b12d-4972-ca4f-342ee9622022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing QASPER samples: 100%|██████████| 50/50 [00:00<00:00, 1477.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extractive QA pairs collected: 136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define metrics - normalization, F1, EM\n",
        "\n",
        "def normalize_for_f1(s):\n",
        "    \"\"\"Lowercase, remove punctuation, articles, and extra whitespace; used for token-level F1.\"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    # remove punctuation\n",
        "    s = \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
        "    # remove articles\n",
        "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
        "    # normalize whitespace\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute token-level F1 between prediction and ground_truth strings.\n",
        "    \"\"\"\n",
        "    pred_tokens = normalize_for_f1(prediction).split()\n",
        "    gt_tokens = normalize_for_f1(ground_truth).split()\n",
        "    if len(pred_tokens) == 0 and len(gt_tokens) == 0:\n",
        "        return 1.0\n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        return 0.0\n",
        "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gt_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def exact_match(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Exact Match per your spec: simple stripped string equality.\n",
        "    \"\"\"\n",
        "    if prediction is None:\n",
        "        prediction = \"\"\n",
        "    if ground_truth is None:\n",
        "        ground_truth = \"\"\n",
        "    return 1.0 if prediction.strip() == ground_truth.strip() else 0.0\n"
      ],
      "metadata": {
        "id": "d1QtCME6MNzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ba7h8ISzR8Jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AirVy_4QR8c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Cell 4: Evaluate both RoBERTa models on the collected extractive QA pairs\n",
        "\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# -----------------------------\n",
        "# Metric helper functions\n",
        "# -----------------------------\n",
        "def normalize_text(s):\n",
        "    \"\"\"Lowercase, remove punctuation/articles/extra whitespace.\"\"\"\n",
        "    import re, string\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"Compute token-level F1 between two strings.\"\"\"\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    gt_tokens = normalize_text(ground_truth).split()\n",
        "    common = set(pred_tokens) & set(gt_tokens)\n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        return int(pred_tokens == gt_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    prec = len(common) / len(pred_tokens)\n",
        "    rec = len(common) / len(gt_tokens)\n",
        "    return 2 * prec * rec / (prec + rec)\n",
        "\n",
        "def exact_match(prediction, ground_truth):\n",
        "    return normalize_text(prediction) == normalize_text(ground_truth)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation function\n",
        "# -----------------------------\n",
        "def evaluate_model(model_name, examples, display_name):\n",
        "    if not examples:\n",
        "        print(f\"\\n{display_name}: No results (model not evaluated).\")\n",
        "        return None\n",
        "\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    print(f\"\\nUsing device: {'cuda' if device == 0 else 'cpu'}\")\n",
        "\n",
        "    print(f\"\\nLoading pipeline for {display_name} ({model_name}) ...\")\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=model_name, tokenizer=model_name, device=device)\n",
        "    print(f\"Device set to use {'cuda:0' if device == 0 else 'cpu'}\")\n",
        "\n",
        "    total_f1, total_em = 0.0, 0.0\n",
        "    count = 0\n",
        "\n",
        "    for ex in tqdm(examples, desc=f\"Evaluating {display_name}\"):\n",
        "        query = ex[\"question\"]\n",
        "        context = ex[\"context\"]\n",
        "        gt = ex[\"ground_truth_answer\"]\n",
        "\n",
        "        try:\n",
        "            result = qa_pipeline(question=query, context=context)\n",
        "            pred = result[\"answer\"].strip()\n",
        "            total_f1 += f1_score(pred, gt)\n",
        "            total_em += exact_match(pred, gt)\n",
        "            count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipped one example due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if count > 0:\n",
        "        avg_f1 = total_f1 / count\n",
        "        avg_em = total_em / count\n",
        "        print(f\"\\n{display_name}: Average F1 = {avg_f1:.4f}, Average EM = {avg_em:.4f} (evaluated on {count} extractive QAs)\")\n",
        "        return avg_f1, avg_em\n",
        "    else:\n",
        "        print(f\"\\n{display_name}: No valid evaluations performed.\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# Run evaluations for both models\n",
        "# -----------------------------\n",
        "print(f\"Evaluating on {len(examples)} examples...\\n\")\n",
        "\n",
        "results_base = evaluate_model(\"deepset/roberta-base-squad2\", examples, \"RoBERTa-base SQuAD2\")\n",
        "results_large = evaluate_model(\"deepset/roberta-large-squad2\", examples, \"RoBERTa-large SQuAD2\")\n",
        "\n",
        "# -----------------------------\n",
        "# Final output\n",
        "# -----------------------------\n",
        "if results_base:\n",
        "    print(f\"\\nRoBERTa-base SQuAD2: Average F1 = {results_base[0]:.4f}, Average EM = {results_base[1]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nRoBERTa-base SQuAD2: No results (model not evaluated).\")\n",
        "\n",
        "if results_large:\n",
        "    print(f\"RoBERTa-large SQuAD2: Average F1 = {results_large[0]:.4f}, Average EM = {results_large[1]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nRoBERTa-large SQuAD2: No results (model not evaluated).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isj7Dl-kMoVm",
        "outputId": "87fe2e77-4ae9-4786-d011-70a09ea05468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on 136 examples...\n",
            "\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "Loading pipeline for RoBERTa-base SQuAD2 (deepset/roberta-base-squad2) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating RoBERTa-base SQuAD2:   7%|▋         | 10/136 [00:06<01:07,  1.87it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Evaluating RoBERTa-base SQuAD2: 100%|██████████| 136/136 [01:03<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RoBERTa-base SQuAD2: Average F1 = 0.1381, Average EM = 0.0588 (evaluated on 136 extractive QAs)\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "Loading pipeline for RoBERTa-large SQuAD2 (deepset/roberta-large-squad2) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating RoBERTa-large SQuAD2: 100%|██████████| 136/136 [03:22<00:00,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RoBERTa-large SQuAD2: Average F1 = 0.1640, Average EM = 0.0515 (evaluated on 136 extractive QAs)\n",
            "\n",
            "RoBERTa-base SQuAD2: Average F1 = 0.1381, Average EM = 0.0588\n",
            "RoBERTa-large SQuAD2: Average F1 = 0.1640, Average EM = 0.0515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpALJB93Rqrt",
        "outputId": "c69c3299-0e6b-4b3e-a1c2-40303162a3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Retrieval + Sliding-window chunking + QA evaluation for top-10 papers per query\n",
        "# Requires: ds (HF dataset of 50 papers), examples (list of extractive QAs)\n",
        "# Outputs Average F1 and EM for both deepset/roberta-base-squad2 and deepset/roberta-large-squad2.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import re, string\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------\n",
        "# Params (tuneable)\n",
        "# -------------------------\n",
        "TOP_K_PAPERS = 10         # top-k papers to search per query\n",
        "CHUNK_SIZE = 400          # chunk size in tokens (model tokens)\n",
        "CHUNK_STRIDE = 128        # overlap stride in tokens\n",
        "MAX_DOCS = len(ds)        # use all loaded papers (should be 50 as loaded)\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# -------------------------\n",
        "# Helpers: normalize / metrics\n",
        "# -------------------------\n",
        "def normalize_for_eval(s):\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
        "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    pred_tokens = normalize_for_eval(prediction).split()\n",
        "    gt_tokens = normalize_for_eval(ground_truth).split()\n",
        "    if len(pred_tokens) == 0 and len(gt_tokens) == 0:\n",
        "        return 1.0\n",
        "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "        return 0.0\n",
        "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gt_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def exact_match(prediction, ground_truth):\n",
        "    return 1.0 if normalize_for_eval(prediction) == normalize_for_eval(ground_truth) else 0.0\n",
        "\n",
        "# -------------------------\n",
        "# Build corpus of paper texts for retrieval\n",
        "# -------------------------\n",
        "def build_paper_text(ex):\n",
        "    # try to include title + abstract + full_text\n",
        "    parts = []\n",
        "    if ex.get(\"title\"):\n",
        "        parts.append(ex[\"title\"])\n",
        "    if ex.get(\"abstract\"):\n",
        "        parts.append(ex[\"abstract\"])\n",
        "    # full_text can be nested - try to flatten common structures\n",
        "    ft = ex.get(\"full_text\", {})\n",
        "    # If earlier concat_full_text exists, use it, else fallback:\n",
        "    try:\n",
        "        full_text_str = concat_full_text(ft)\n",
        "    except Exception:\n",
        "        # robust fallback: join paragraphs if present\n",
        "        if isinstance(ft, dict) and \"paragraphs\" in ft:\n",
        "            paragraphs = []\n",
        "            for p in ft[\"paragraphs\"]:\n",
        "                if isinstance(p, list):\n",
        "                    paragraphs.append(\" \".join(p))\n",
        "                elif isinstance(p, str):\n",
        "                    paragraphs.append(p)\n",
        "            full_text_str = \" \".join(paragraphs)\n",
        "        elif isinstance(ft, list):\n",
        "            # list of lists or strings\n",
        "            segments = []\n",
        "            for p in ft:\n",
        "                if isinstance(p, list):\n",
        "                    segments.append(\" \".join(p))\n",
        "                else:\n",
        "                    segments.append(str(p))\n",
        "            full_text_str = \" \".join(segments)\n",
        "        else:\n",
        "            full_text_str = str(ft)\n",
        "    parts.append(full_text_str)\n",
        "    return \" \".join([p for p in parts if p])\n",
        "\n",
        "paper_texts = []\n",
        "paper_ids = []\n",
        "for i, ex in enumerate(ds):\n",
        "    paper_ids.append(ex.get(\"id\", f\"paper_{i}\"))\n",
        "    paper_texts.append(build_paper_text(ex))\n",
        "\n",
        "# -------------------------\n",
        "# TF-IDF retriever (simple & effective)\n",
        "# -------------------------\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=20000)\n",
        "tfidf_matrix = vectorizer.fit_transform(paper_texts)  # (num_papers x vocab)\n",
        "\n",
        "# Map queries (we evaluate each example separately)\n",
        "queries = [ex[\"question\"] for ex in examples]\n",
        "\n",
        "# Precompute query vectors\n",
        "query_vecs = vectorizer.transform(queries)  # (num_queries x vocab)\n",
        "\n",
        "# For each query, compute cosine similarity to papers and select top-k papers\n",
        "# This produces a list topk_paper_indices_per_query of length len(queries)\n",
        "cosine_sim = cosine_similarity(query_vecs, tfidf_matrix)  # (num_queries x num_papers)\n",
        "topk_paper_indices_per_query = np.argsort(-cosine_sim, axis=1)[:, :TOP_K_PAPERS]\n",
        "\n",
        "# -------------------------\n",
        "# Chunking utility (token-based) using tokenizer\n",
        "# -------------------------\n",
        "def chunk_text_tokenwise(text, tokenizer, chunk_size=CHUNK_SIZE, stride=CHUNK_STRIDE):\n",
        "    # encode -> list of token ids, split into overlapping windows, decode windows back to text\n",
        "    tok_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(tok_ids) == 0:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tok_ids):\n",
        "        end = min(start + chunk_size, len(tok_ids))\n",
        "        chunk_ids = tok_ids[start:end]\n",
        "        # decode chunk back to text\n",
        "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "        chunks.append(chunk_text)\n",
        "        if end == len(tok_ids):\n",
        "            break\n",
        "        start += (chunk_size - stride)\n",
        "    return chunks\n",
        "\n",
        "# -------------------------\n",
        "# Evaluate function (retrieval + chunking + QA)\n",
        "# -------------------------\n",
        "def evaluate_with_retrieval_and_chunking(model_name, examples, topk_indices_per_query, paper_texts, device):\n",
        "    # Create pipeline and tokenizer\n",
        "    device_idx = 0 if device else -1\n",
        "    qa_pipe = pipeline(\"question-answering\", model=model_name, tokenizer=model_name, device=device_idx)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    total_f1 = 0.0\n",
        "    total_em = 0.0\n",
        "    n = 0\n",
        "\n",
        "    # iterate examples and run retrieval+chunking\n",
        "    for qi, ex in enumerate(tqdm(examples, desc=f\"Retrieval+Chunking eval for {model_name}\")):\n",
        "        query = ex[\"question\"]\n",
        "        gold = ex[\"ground_truth_answer\"]\n",
        "        # Top-K paper indices for this query\n",
        "        topk = topk_indices_per_query[qi]\n",
        "        best_pred = \"\"\n",
        "        best_score = -1.0\n",
        "        found_any = False\n",
        "\n",
        "        # For each candidate paper, chunk and run QA\n",
        "        for pidx in topk:\n",
        "            doc_text = paper_texts[pidx]\n",
        "            if not doc_text or len(doc_text.strip()) == 0:\n",
        "                continue\n",
        "            chunks = chunk_text_tokenwise(doc_text, tokenizer, chunk_size=CHUNK_SIZE, stride=CHUNK_STRIDE)\n",
        "            if not chunks:\n",
        "                continue\n",
        "            for chunk in chunks:\n",
        "                try:\n",
        "                    out = qa_pipe(question=query, context=chunk)\n",
        "                    # pipeline returns dict. If list returned, take the first\n",
        "                    if isinstance(out, list):\n",
        "                        out = out[0] if len(out) > 0 else {\"answer\": \"\", \"score\": 0.0}\n",
        "                    ans = out.get(\"answer\", \"\").strip()\n",
        "                    score = out.get(\"score\", 0.0) or 0.0\n",
        "                    # keep best by score\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_pred = ans\n",
        "                        found_any = True\n",
        "                except Exception as e:\n",
        "                    # skip chunk on exception but don't crash whole evaluation\n",
        "                    # print(\"Chunk error:\", e)\n",
        "                    continue\n",
        "\n",
        "        # If we didn't get anything (should not happen), set pred empty\n",
        "        if not found_any:\n",
        "            best_pred = \"\"\n",
        "\n",
        "        # compute metrics (even if empty predicted)\n",
        "        total_f1 += f1_score(best_pred, gold)\n",
        "        total_em += exact_match(best_pred, gold)\n",
        "        n += 1\n",
        "\n",
        "    # cleanup\n",
        "    try:\n",
        "        del qa_pipe\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    avg_f1 = (total_f1 / n) if n > 0 else 0.0\n",
        "    avg_em = (total_em / n) if n > 0 else 0.0\n",
        "    return avg_f1, avg_em, n\n",
        "\n",
        "# -------------------------\n",
        "# Run evaluation for both models\n",
        "# -------------------------\n",
        "device_flag = True if USE_CUDA else False\n",
        "print(\"Device available:\", \"cuda\" if device_flag else \"cpu\")\n",
        "\n",
        "models = [\n",
        "    (\"RoBERTa-base SQuAD2\", \"deepset/roberta-base-squad2\"),\n",
        "    (\"RoBERTa-large SQuAD2\", \"deepset/roberta-large-squad2\")\n",
        "]\n",
        "\n",
        "results = {}\n",
        "for display_name, model_id in models:\n",
        "    print(f\"\\nStarting evaluation for {display_name} ({model_id}) ...\")\n",
        "    avg_f1, avg_em, count = evaluate_with_retrieval_and_chunking(model_id, examples, topk_paper_indices_per_query, paper_texts, device_flag)\n",
        "    results[display_name] = {\"avg_f1\": avg_f1, \"avg_em\": avg_em, \"n\": count}\n",
        "\n",
        "# -------------------------\n",
        "# Print final results\n",
        "# -------------------------\n",
        "for display_name in [\"RoBERTa-base SQuAD2\", \"RoBERTa-large SQuAD2\"]:\n",
        "    if display_name in results:\n",
        "        r = results[display_name]\n",
        "        print(f\"\\n{display_name}: Average F1 = {r['avg_f1']:.4f}, Average EM = {r['avg_em']:.4f} (evaluated on {r['n']} extractive QAs)\")\n",
        "    else:\n",
        "        print(f\"\\n{display_name}: No results (model not evaluated).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45-6icNucirq",
        "outputId": "5e6b9b44-13fe-4c3d-ddef-2fedfad161c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device available: cuda\n",
            "\n",
            "Starting evaluation for RoBERTa-base SQuAD2 (deepset/roberta-base-squad2) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Retrieval+Chunking eval for deepset/roberta-base-squad2:   0%|          | 0/136 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (7632 > 512). Running this sequence through the model will result in indexing errors\n",
            "Retrieval+Chunking eval for deepset/roberta-base-squad2: 100%|██████████| 136/136 [15:12<00:00,  6.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting evaluation for RoBERTa-large SQuAD2 (deepset/roberta-large-squad2) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Retrieval+Chunking eval for deepset/roberta-large-squad2:   0%|          | 0/136 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (7632 > 512). Running this sequence through the model will result in indexing errors\n",
            "Retrieval+Chunking eval for deepset/roberta-large-squad2: 100%|██████████| 136/136 [46:45<00:00, 20.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RoBERTa-base SQuAD2: Average F1 = 0.0531, Average EM = 0.0221 (evaluated on 136 extractive QAs)\n",
            "\n",
            "RoBERTa-large SQuAD2: Average F1 = 0.0822, Average EM = 0.0294 (evaluated on 136 extractive QAs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpLBw2g6wG9Q",
        "outputId": "316b8b30-136a-4c7b-afd8-c641da224d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        }
      ]
    }
  ]
}