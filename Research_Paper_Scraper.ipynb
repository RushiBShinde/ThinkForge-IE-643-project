{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RushiBShinde/ThinkForge-IE-643-project/blob/main/Research_Paper_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_paper(url):\n",
        "    \"\"\"\n",
        "    Scrapes a research paper page for its title, authors, abstract, and full text content.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the research paper.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paper's metadata and full text,\n",
        "        or None if scraping fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # --- Clean the soup by removing irrelevant tags ---\n",
        "        # This helps in removing common website elements that are not part of the paper's content.\n",
        "        for tag in soup(['nav', 'header', 'footer', 'aside', 'script', 'style']):\n",
        "            tag.decompose()\n",
        "\n",
        "        # --- Find Paper Title ---\n",
        "        # Common tags for titles are <h1> or <meta property=\"og:title\">\n",
        "        title_tag = soup.find('h1', {'class': 'title'}) or soup.find('meta', property='og:title')\n",
        "        title = title_tag['content'] if title_tag and title_tag.has_attr('content') else (title_tag.get_text(strip=True) if title_tag else \"Title not found\")\n",
        "\n",
        "\n",
        "        # --- Find Authors ---\n",
        "        # This is highly variable. We'll try a few common patterns.\n",
        "        authors_list = []\n",
        "        # Pattern 1: Meta tag for authors\n",
        "        author_tags = soup.find_all('meta', {'name': 'citation_author'})\n",
        "        if author_tags:\n",
        "            authors_list = [tag['content'] for tag in author_tags]\n",
        "        else:\n",
        "            # Pattern 2: Specific class names (these are examples, will need adjustment)\n",
        "            author_div = soup.find('div', class_='authors')\n",
        "            if author_div:\n",
        "                authors_list = [a.get_text(strip=True) for a in author_div.find_all('a')]\n",
        "\n",
        "        authors = \", \".join(authors_list) if authors_list else \"Authors not found\"\n",
        "\n",
        "        # --- Find Abstract ---\n",
        "        # Abstracts are often in a <div> with a specific class or heading\n",
        "        abstract_heading = soup.find(['h2', 'h3'], string=lambda t: t and 'abstract' in t.lower())\n",
        "        abstract = \"Abstract not found\"\n",
        "        if abstract_heading:\n",
        "            abstract_tag = abstract_heading.find_next('div', {'class': 'abstract-content'}) or abstract_heading.find_next('p')\n",
        "            if abstract_tag:\n",
        "                 abstract = abstract_tag.get_text(strip=True)\n",
        "\n",
        "        # --- Find Full Text ---\n",
        "        # This is a generic approach to get all text. It might include headers, footers, etc.\n",
        "        # For more precision, you'd need to identify the main content container for each site.\n",
        "        full_text = \"Full text not found\"\n",
        "        # A common tag for the main content is <article> or a div with id='content' or role='main'\n",
        "        content_area = soup.find('article') or soup.find('div', id='content') or soup.find('div', class_='content') or soup.find('div', role='main')\n",
        "        if content_area:\n",
        "            full_text = content_area.get_text(separator='\\n', strip=True)\n",
        "        else:\n",
        "            # Fallback to getting all text from the body, which is now cleaner after decomposition\n",
        "            body_tag = soup.find('body')\n",
        "            if body_tag:\n",
        "                full_text = body_tag.get_text(separator='\\n', strip=True)\n",
        "\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'abstract': abstract,\n",
        "            'full_text': full_text,\n",
        "            'url': url\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_links_from_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads a file of URLs, scrapes each one, and returns the data.\n",
        "\n",
        "    Args:\n",
        "        filepath: The path to the text or csv file with URLs.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary is a scraped paper.\n",
        "    \"\"\"\n",
        "    scraped_data = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        # Read either as plain text or from the first column of a CSV\n",
        "        reader = csv.reader(f)\n",
        "        urls = [row[0] for row in reader] if '.' in f.name and f.name.rsplit('.', 1)[1].lower() == 'csv' else f.read().splitlines()\n",
        "\n",
        "\n",
        "    for url in urls:\n",
        "        if url.strip(): # Ensure the line is not empty\n",
        "            print(f\"Scraping: {url}\")\n",
        "            data = scrape_paper(url)\n",
        "            if data:\n",
        "                scraped_data.append(data)\n",
        "    return scraped_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create a dummy links file for testing\n",
        "    with open(\"paper_links.txt\", \"w\") as f:\n",
        "        f.write(\"https://arxiv.org/abs/1706.03762\\n\") # Attention is All You Need\n",
        "        f.write(\"https://arxiv.org/abs/2106.07682\\n\") # Paper on Vision Transformers\n",
        "        f.write(\"https://www.ijpsjournal.com/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation\") # Example of a failed URL\n",
        "\n",
        "    # 1. Process the file of links\n",
        "    papers = process_links_from_file('paper_links.txt')\n",
        "\n",
        "    if papers:\n",
        "        # 2. Save the results to a CSV file\n",
        "        output_filename = 'scraped_papers.csv'\n",
        "        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['title', 'authors', 'abstract', 'full_text', 'url']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for paper in papers:\n",
        "                writer.writerow(paper)\n",
        "\n",
        "        print(f\"\\nScraping complete. Data saved to {output_filename}\")\n",
        "        print(f\"Successfully scraped {len(papers)} papers.\")\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://arxiv.org/abs/1706.03762\n",
            "Scraping: https://arxiv.org/abs/2106.07682\n",
            "Scraping: https://www.ijpsjournal.com/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation\n",
            "\n",
            "Scraping complete. Data saved to scraped_papers.csv\n",
            "Successfully scraped 3 papers.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSMk9-rR7dtr",
        "outputId": "8080a0c4-5f0d-4e9a-8b9e-9b1545d99240"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import fitz  # PyMuPDF - install with 'pip install PyMuPDF'\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def extract_text_from_pdf(pdf_content):\n",
        "    \"\"\"\n",
        "    Extracts text from the raw content of a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_content: The byte content of the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        A string containing all the text from the PDF, or an empty string if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the PDF from memory\n",
        "        with fitz.open(stream=pdf_content, filetype=\"pdf\") as doc:\n",
        "            full_text = \"\"\n",
        "            for page in doc:\n",
        "                full_text += page.get_text()\n",
        "            return full_text\n",
        "    except Exception as e:\n",
        "        print(f\"  - Could not extract text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def scrape_paper(url):\n",
        "    \"\"\"\n",
        "    Scrapes a research paper page for its title, authors, abstract, and full text content.\n",
        "    It prioritizes finding and extracting text from a linked PDF.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the research paper.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paper's metadata and full text,\n",
        "        or None if scraping fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # --- Clean the soup by removing irrelevant tags ---\n",
        "        # This helps in removing common website elements that are not part of the paper's content.\n",
        "        for tag in soup(['nav', 'header', 'footer', 'aside', 'script', 'style']):\n",
        "            tag.decompose()\n",
        "\n",
        "        # --- Find Paper Title ---\n",
        "        # Common tags for titles are <h1> or <meta property=\"og:title\">\n",
        "        title_tag = soup.find('h1', {'class': 'title'}) or soup.find('meta', property='og:title')\n",
        "        title = title_tag['content'] if title_tag and title_tag.has_attr('content') else (title_tag.get_text(strip=True) if title_tag else \"Title not found\")\n",
        "\n",
        "\n",
        "        # --- Find Authors ---\n",
        "        # This is highly variable. We'll try a few common patterns.\n",
        "        authors_list = []\n",
        "        # Pattern 1: Meta tag for authors\n",
        "        author_tags = soup.find_all('meta', {'name': 'citation_author'})\n",
        "        if author_tags:\n",
        "            authors_list = [tag['content'] for tag in author_tags]\n",
        "        else:\n",
        "            # Pattern 2: Specific class names (these are examples, will need adjustment)\n",
        "            author_div = soup.find('div', class_='authors')\n",
        "            if author_div:\n",
        "                authors_list = [a.get_text(strip=True) for a in author_div.find_all('a')]\n",
        "\n",
        "        authors = \", \".join(authors_list) if authors_list else \"Authors not found\"\n",
        "\n",
        "        # --- Find Abstract ---\n",
        "        # Abstracts are often in a <div> with a specific class or heading\n",
        "        abstract_heading = soup.find(['h2', 'h3'], string=lambda t: t and 'abstract' in t.lower())\n",
        "        abstract = \"Abstract not found\"\n",
        "        if abstract_heading:\n",
        "            abstract_tag = abstract_heading.find_next('div', {'class': 'abstract-content'}) or abstract_heading.find_next('p')\n",
        "            if abstract_tag:\n",
        "                 abstract = abstract_tag.get_text(strip=True)\n",
        "\n",
        "        full_text = \"Full text not found\"\n",
        "\n",
        "        # --- Find and Process PDF ---\n",
        "        # Look for links that point to a PDF file.\n",
        "        pdf_links = soup.find_all('a', href=True)\n",
        "        for link in pdf_links:\n",
        "            href = link['href']\n",
        "            # Check if the link is likely a PDF link\n",
        "            if href.lower().endswith('.pdf') or 'download pdf' in link.get_text(strip=True).lower():\n",
        "                # Construct the absolute URL for the PDF\n",
        "                pdf_url = urljoin(url, href)\n",
        "                print(f\"  - Found potential PDF link: {pdf_url}\")\n",
        "                try:\n",
        "                    # Download the PDF\n",
        "                    pdf_response = requests.get(pdf_url, headers=headers, timeout=10)\n",
        "                    pdf_response.raise_for_status()\n",
        "\n",
        "                    # Extract text from the downloaded PDF content\n",
        "                    extracted_text = extract_text_from_pdf(pdf_response.content)\n",
        "                    if extracted_text:\n",
        "                        print(\"  - Successfully extracted text from PDF.\")\n",
        "                        full_text = extracted_text\n",
        "                        break # Stop after the first successfully processed PDF\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"  - Failed to download PDF from {pdf_url}: {e}\")\n",
        "                    continue # Try the next link\n",
        "\n",
        "        # --- Fallback to Full Text from HTML ---\n",
        "        # This will only run if no PDF was found or processed successfully.\n",
        "        if full_text == \"Full text not found\":\n",
        "            print(\"  - No usable PDF found. Falling back to scraping HTML body.\")\n",
        "            content_area = soup.find('article') or soup.find('div', id='content') or soup.find('div', class_='content') or soup.find('div', role='main')\n",
        "            if content_area:\n",
        "                full_text = content_area.get_text(separator='\\n', strip=True)\n",
        "            else:\n",
        "                # Fallback to getting all text from the body, which is now cleaner after decomposition\n",
        "                body_tag = soup.find('body')\n",
        "                if body_tag:\n",
        "                    full_text = body_tag.get_text(separator='\\n', strip=True)\n",
        "\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'abstract': abstract,\n",
        "            'full_text': full_text,\n",
        "            'url': url\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_links_from_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads a file of URLs, scrapes each one, and returns the data.\n",
        "\n",
        "    Args:\n",
        "        filepath: The path to the text or csv file with URLs.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary is a scraped paper.\n",
        "    \"\"\"\n",
        "    scraped_data = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        # Read either as plain text or from the first column of a CSV\n",
        "        reader = csv.reader(f)\n",
        "        urls = [row[0] for row in reader] if '.' in f.name and f.name.rsplit('.', 1)[1].lower() == 'csv' else f.read().splitlines()\n",
        "\n",
        "\n",
        "    for url in urls:\n",
        "        if url.strip(): # Ensure the line is not empty\n",
        "            print(f\"Scraping: {url}\")\n",
        "            data = scrape_paper(url)\n",
        "            if data:\n",
        "                scraped_data.append(data)\n",
        "    return scraped_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create a dummy links file for testing\n",
        "    # Note: arXiv pages have a \".pdf\" link right on the abstract page.\n",
        "    with open(\"paper_links.txt\", \"w\") as f:\n",
        "        f.write(\"https://arxiv.org/abs/1706.03762\\n\") # Attention is All You Need\n",
        "        f.write(\"https://arxiv.org/abs/2106.07682\\n\") # Paper on Vision Transformers\n",
        "        f.write(\"https://www.ijpsjournal.com/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation\") # Example of a failed URL\n",
        "\n",
        "    # 1. Process the file of links\n",
        "    papers = process_links_from_file('paper_links.txt')\n",
        "\n",
        "    if papers:\n",
        "        # 2. Save the results to a CSV file\n",
        "        output_filename = 'scraped_papers.csv'\n",
        "        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['title', 'authors', 'abstract', 'full_text', 'url']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for paper in papers:\n",
        "                writer.writerow(paper)\n",
        "\n",
        "        print(f\"\\nScraping complete. Data saved to {output_filename}\")\n",
        "        print(f\"Successfully scraped {len(papers)} papers.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saIDDajZAk9A",
        "outputId": "4badbeec-0cdb-4699-f4be-3d8cf6f3faab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://arxiv.org/abs/1706.03762\n",
            "  - No usable PDF found. Falling back to scraping HTML body.\n",
            "Scraping: https://arxiv.org/abs/2106.07682\n",
            "  - No usable PDF found. Falling back to scraping HTML body.\n",
            "Scraping: https://www.ijpsjournal.com/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation\n",
            "  - Found potential PDF link: https://www.ijpsjournal.com/assetsbackoffice/uploads/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation.pdf\n",
            "  - Successfully extracted text from PDF.\n",
            "\n",
            "Scraping complete. Data saved to scraped_papers.csv\n",
            "Successfully scraped 3 papers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above scrapes pdfs from unprotected sites. but there are some sites that are anti scraping."
      ],
      "metadata": {
        "id": "--ILw95aDLID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import fitz  # PyMuPDF - install with 'pip install PyMuPDF'\n",
        "from urllib.parse import urljoin\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "import time\n",
        "\n",
        "def get_page_source_with_selenium(url):\n",
        "    \"\"\"\n",
        "    Fetches the page source of a URL using a headless Selenium browser\n",
        "    to bypass anti-scraping measures. CONFIGURED FOR GOOGLE COLAB.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to fetch.\n",
        "\n",
        "    Returns:\n",
        "        The page's HTML source as a string, or None if it fails.\n",
        "    \"\"\"\n",
        "    chrome_options = Options()\n",
        "    # Add options to run in a Colab environment\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\")\n",
        "\n",
        "    try:\n",
        "        # The service object is not needed when running in Colab this way\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        driver.get(url)\n",
        "        # Wait for dynamically loaded content\n",
        "        time.sleep(3)\n",
        "        page_source = driver.page_source\n",
        "        driver.quit()\n",
        "        return page_source\n",
        "    except WebDriverException as e:\n",
        "        print(f\"  - Selenium error: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"  - An unexpected error occurred with Selenium: {e}\")\n",
        "        if 'driver' in locals() and driver:\n",
        "            driver.quit()\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_content):\n",
        "    \"\"\"\n",
        "    Extracts text from the raw content of a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_content: The byte content of the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        A string containing all the text from the PDF, or an empty string if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the PDF from memory\n",
        "        with fitz.open(stream=pdf_content, filetype=\"pdf\") as doc:\n",
        "            full_text = \"\"\n",
        "            for page in doc:\n",
        "                full_text += page.get_text()\n",
        "            return full_text\n",
        "    except Exception as e:\n",
        "        print(f\"  - Could not extract text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def scrape_paper(url):\n",
        "    \"\"\"\n",
        "    Scrapes a research paper page for its title, authors, abstract, and full text content.\n",
        "    It prioritizes finding and extracting text from a linked PDF.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the research paper.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paper's metadata and full text,\n",
        "        or None if scraping fails.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "    }\n",
        "\n",
        "    page_source = get_page_source_with_selenium(url)\n",
        "    if not page_source:\n",
        "        print(f\"Error: Could not retrieve page source for {url}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # --- Clean the soup by removing irrelevant tags ---\n",
        "        for tag in soup(['nav', 'header', 'footer', 'aside', 'script', 'style']):\n",
        "            tag.decompose()\n",
        "\n",
        "        # --- Find Paper Title ---\n",
        "        title_tag = soup.find('h1', {'class': 'title'}) or soup.find('meta', property='og:title')\n",
        "        title = title_tag['content'] if title_tag and title_tag.has_attr('content') else (title_tag.get_text(strip=True) if title_tag else \"Title not found\")\n",
        "\n",
        "        # --- Find Authors ---\n",
        "        authors_list = []\n",
        "        author_tags = soup.find_all('meta', {'name': 'citation_author'})\n",
        "        if author_tags:\n",
        "            authors_list = [tag['content'] for tag in author_tags]\n",
        "        else:\n",
        "            author_div = soup.find('div', class_='authors')\n",
        "            if author_div:\n",
        "                authors_list = [a.get_text(strip=True) for a in author_div.find_all('a')]\n",
        "\n",
        "        authors = \", \".join(authors_list) if authors_list else \"Authors not found\"\n",
        "\n",
        "        # --- Find Abstract ---\n",
        "        abstract_heading = soup.find(['h2', 'h3'], string=lambda t: t and 'abstract' in t.lower())\n",
        "        abstract = \"Abstract not found\"\n",
        "        if abstract_heading:\n",
        "            abstract_tag = abstract_heading.find_next('div', {'class': 'abstract-content'}) or abstract_heading.find_next('p')\n",
        "            if abstract_tag:\n",
        "                 abstract = abstract_tag.get_text(strip=True)\n",
        "\n",
        "        full_text = \"Full text not found\"\n",
        "\n",
        "        # --- Find and Process PDF ---\n",
        "        pdf_meta_tag = soup.find('meta', {'name': 'citation_pdf_url'})\n",
        "        if pdf_meta_tag and pdf_meta_tag.has_attr('content'):\n",
        "            pdf_url_from_meta = pdf_meta_tag['content']\n",
        "            print(f\"  - Found PDF meta tag: {pdf_url_from_meta}\")\n",
        "            try:\n",
        "                pdf_response = requests.get(pdf_url_from_meta, headers=headers, timeout=15)\n",
        "                pdf_response.raise_for_status()\n",
        "                extracted_text = extract_text_from_pdf(pdf_response.content)\n",
        "                if extracted_text:\n",
        "                    print(\"  - Successfully extracted text from PDF via meta tag.\")\n",
        "                    full_text = extracted_text\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"  - Failed to download PDF from meta tag URL {pdf_url_from_meta}: {e}\")\n",
        "\n",
        "        if full_text == \"Full text not found\":\n",
        "            print(\"  - No usable PDF found via meta tag. Searching for links on page.\")\n",
        "            pdf_links = soup.find_all('a', href=True)\n",
        "            for link in pdf_links:\n",
        "                href = link['href']\n",
        "                if href.lower().endswith('.pdf') or 'download pdf' in link.get_text(strip=True).lower():\n",
        "                    pdf_url = urljoin(url, href)\n",
        "                    print(f\"  - Found potential PDF link: {pdf_url}\")\n",
        "                    try:\n",
        "                        pdf_response = requests.get(pdf_url, headers=headers, timeout=10)\n",
        "                        pdf_response.raise_for_status()\n",
        "                        extracted_text = extract_text_from_pdf(pdf_response.content)\n",
        "                        if extracted_text:\n",
        "                            print(\"  - Successfully extracted text from PDF.\")\n",
        "                            full_text = extracted_text\n",
        "                            break\n",
        "                    except requests.exceptions.RequestException as e:\n",
        "                        print(f\"  - Failed to download PDF from {pdf_url}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        if full_text == \"Full text not found\":\n",
        "            print(\"  - No usable PDF found. Falling back to scraping HTML body.\")\n",
        "            content_area = soup.find('article') or soup.find('div', id='content') or soup.find('div', class_='content') or soup.find('div', role='main')\n",
        "            if content_area:\n",
        "                full_text = content_area.get_text(separator='\\n', strip=True)\n",
        "            else:\n",
        "                body_tag = soup.find('body')\n",
        "                if body_tag:\n",
        "                    full_text = body_tag.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'abstract': abstract,\n",
        "            'full_text': full_text,\n",
        "            'url': url\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while parsing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_links_from_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads a file of URLs, scrapes each one, and returns the data.\n",
        "\n",
        "    Args:\n",
        "        filepath: The path to the text or csv file with URLs.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary is a scraped paper.\n",
        "    \"\"\"\n",
        "    scraped_data = []\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            # Read either as plain text or from the first column of a CSV\n",
        "            is_csv = '.csv' in filepath.lower()\n",
        "            if is_csv:\n",
        "                reader = csv.reader(f)\n",
        "                urls = [row[0] for row in reader if row]\n",
        "            else:\n",
        "                urls = f.read().splitlines()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "    for url in urls:\n",
        "        if url.strip(): # Ensure the line is not empty\n",
        "            print(f\"Scraping: {url}\")\n",
        "            data = scrape_paper(url)\n",
        "            if data:\n",
        "                scraped_data.append(data)\n",
        "    return scraped_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create a dummy links file for testing\n",
        "    # Note: arXiv pages have a \".pdf\" link right on the abstract page.\n",
        "    with open(\"paper_links.txt\", \"w\") as f:\n",
        "        f.write(\"https://arxiv.org/abs/1706.03762\\n\") # Attention is All You Need\n",
        "        f.write(\"https://www.sciencedirect.com/science/article/pii/S2213846323001116\\n\") # ScienceDirect Paper\n",
        "        f.write(\"https://www.ijpsjournal.com/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation\") # Example of a failed URL\n",
        "\n",
        "    # 1. Process the file of links\n",
        "    papers = process_links_from_file('paper_links.txt')\n",
        "\n",
        "    if papers:\n",
        "        # 2. Save the results to a CSV file\n",
        "        output_filename = 'scraped_papers.csv'\n",
        "        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['title', 'authors', 'abstract', 'full_text', 'url']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for paper in papers:\n",
        "                writer.writerow(paper)\n",
        "\n",
        "        print(f\"\\nScraping complete. Data saved to {output_filename}\")\n",
        "        print(f\"Successfully scraped {len(papers)} papers.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-X5GvdfDV6q",
        "outputId": "4684411c-d863-4d24-daf2-8568439e6855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://arxiv.org/abs/1706.03762\n",
            "  - Found PDF meta tag: http://arxiv.org/pdf/1706.03762\n",
            "  - Successfully extracted text from PDF via meta tag.\n",
            "Scraping: https://www.sciencedirect.com/science/article/pii/S2213846323001116\n",
            "  - No usable PDF found via meta tag. Searching for links on page.\n",
            "  - No usable PDF found. Falling back to scraping HTML body.\n",
            "Scraping: https://www.ijpsjournal.com/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation\n",
            "  - Found PDF meta tag: https://www.ijpsjournal.com/assetsbackoffice/uploads/article/Polypharmacy+A+Review+of+Adverse+Drug+Reaction+Interaction+and+Mitigation.pdf\n",
            "  - Successfully extracted text from PDF via meta tag.\n",
            "\n",
            "Scraping complete. Data saved to scraped_papers.csv\n",
            "Successfully scraped 3 papers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try\n"
      ],
      "metadata": {
        "id": "ZUuMdJpzYQ1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import fitz  # PyMuPDF - install with 'pip install PyMuPDF'\n",
        "from urllib.parse import urljoin\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "import time\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "def get_page_source_with_selenium(url):\n",
        "    \"\"\"\n",
        "    Fetches the page source of a URL using a headless Selenium browser\n",
        "    to bypass anti-scraping measures. CONFIGURED FOR GOOGLE COLAB.\n",
        "\n",
        "    Args:\n",
        "        url: The URL to fetch.\n",
        "\n",
        "    Returns:\n",
        "        The page's HTML source as a string, or None if it fails.\n",
        "    \"\"\"\n",
        "    chrome_options = Options()\n",
        "    # Add options to run in a Colab environment\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\")\n",
        "\n",
        "    driver = None\n",
        "    try:\n",
        "        # The service object is not needed when running in Colab this way\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        driver.get(url)\n",
        "\n",
        "        # Use an explicit wait for better reliability instead of a fixed sleep.\n",
        "        # Wait up to 20 seconds for an element with id 'body' to be present.\n",
        "        # This ensures the page's JavaScript has loaded the main content.\n",
        "        wait = WebDriverWait(driver, 20)\n",
        "        wait.until(EC.presence_of_element_located((By.ID, \"body\")))\n",
        "\n",
        "        page_source = driver.page_source\n",
        "        driver.quit()\n",
        "        return page_source\n",
        "    except WebDriverException as e:\n",
        "        print(f\"  - Selenium error: {e}\")\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"  - An unexpected error occurred with Selenium: {e}\")\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_content):\n",
        "    \"\"\"\n",
        "    Extracts text from the raw content of a PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_content: The byte content of the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        A string containing all the text from the PDF, or an empty string if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the PDF from memory\n",
        "        with fitz.open(stream=pdf_content, filetype=\"pdf\") as doc:\n",
        "            full_text = \"\"\n",
        "            for page in doc:\n",
        "                full_text += page.get_text()\n",
        "            return full_text\n",
        "    except Exception as e:\n",
        "        print(f\"  - Could not extract text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def scrape_paper(url):\n",
        "    \"\"\"\n",
        "    Scrapes a research paper page for its title, authors, abstract, and full text content.\n",
        "    It prioritizes finding and extracting text from a linked PDF.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the research paper.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the paper's metadata and full text,\n",
        "        or None if scraping fails.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "    }\n",
        "\n",
        "    page_source = get_page_source_with_selenium(url)\n",
        "    if not page_source:\n",
        "        print(f\"Error: Could not retrieve page source for {url}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # --- Clean the soup by removing irrelevant tags ---\n",
        "        for tag in soup(['nav', 'header', 'footer', 'aside', 'script', 'style']):\n",
        "            tag.decompose()\n",
        "\n",
        "        # --- Find Paper Title ---\n",
        "        title_tag = soup.find('h1', {'class': 'title'}) or soup.find('meta', property='og:title')\n",
        "        title = title_tag['content'] if title_tag and title_tag.has_attr('content') else (title_tag.get_text(strip=True) if title_tag else \"Title not found\")\n",
        "\n",
        "        # --- Find Authors ---\n",
        "        authors_list = []\n",
        "        author_tags = soup.find_all('meta', {'name': 'citation_author'})\n",
        "        if author_tags:\n",
        "            authors_list = [tag['content'] for tag in author_tags]\n",
        "        else:\n",
        "            author_div = soup.find('div', class_='authors')\n",
        "            if author_div:\n",
        "                authors_list = [a.get_text(strip=True) for a in author_div.find_all('a')]\n",
        "\n",
        "        authors = \", \".join(authors_list) if authors_list else \"Authors not found\"\n",
        "\n",
        "        # --- Find Abstract ---\n",
        "        abstract_heading = soup.find(['h2', 'h3'], string=lambda t: t and 'abstract' in t.lower())\n",
        "        abstract = \"Abstract not found\"\n",
        "        if abstract_heading:\n",
        "            abstract_tag = abstract_heading.find_next('div', {'class': 'abstract-content'}) or abstract_heading.find_next('p')\n",
        "            if abstract_tag:\n",
        "                 abstract = abstract_tag.get_text(strip=True)\n",
        "\n",
        "        full_text = \"Full text not found\"\n",
        "\n",
        "        # --- Find and Process PDF ---\n",
        "        # Method 1: Look for the citation_pdf_url meta tag (often the best source)\n",
        "        pdf_meta_tag = soup.find('meta', {'name': 'citation_pdf_url'})\n",
        "        if pdf_meta_tag and pdf_meta_tag.has_attr('content'):\n",
        "            pdf_url_from_meta = pdf_meta_tag['content']\n",
        "            print(f\"  - Found PDF meta tag: {pdf_url_from_meta}\")\n",
        "            try:\n",
        "                pdf_response = requests.get(pdf_url_from_meta, headers=headers, timeout=15)\n",
        "                pdf_response.raise_for_status()\n",
        "                extracted_text = extract_text_from_pdf(pdf_response.content)\n",
        "                if extracted_text:\n",
        "                    print(\"  - Successfully extracted text from PDF via meta tag.\")\n",
        "                    full_text = extracted_text\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"  - Failed to download PDF from meta tag URL {pdf_url_from_meta}: {e}\")\n",
        "\n",
        "        # Method 2: Specifically look for ScienceDirect's PDF button if the first method fails\n",
        "        if full_text == \"Full text not found\":\n",
        "            pdf_button = soup.find('a', id='pdfLink')\n",
        "            if pdf_button and pdf_button.has_attr('href'):\n",
        "                pdf_url = urljoin(url, pdf_button['href'])\n",
        "                print(f\"  - Found specific PDF button (e.g., ScienceDirect): {pdf_url}\")\n",
        "                try:\n",
        "                    pdf_response = requests.get(pdf_url, headers=headers, timeout=15)\n",
        "                    pdf_response.raise_for_status()\n",
        "                    if 'application/pdf' in pdf_response.headers.get('Content-Type', ''):\n",
        "                        extracted_text = extract_text_from_pdf(pdf_response.content)\n",
        "                        if extracted_text:\n",
        "                            print(\"  - Successfully extracted text from PDF button link.\")\n",
        "                            full_text = extracted_text\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"  - Failed to download PDF from button link {pdf_url}: {e}\")\n",
        "\n",
        "        # Method 3: Fallback to searching all links on the page\n",
        "        if full_text == \"Full text not found\":\n",
        "            print(\"  - No usable PDF found yet. Searching all links on page as a fallback.\")\n",
        "            pdf_links = soup.find_all('a', href=True)\n",
        "            processed_urls = set()\n",
        "\n",
        "            for link in pdf_links:\n",
        "                href = link['href']\n",
        "                pdf_url = urljoin(url, href)\n",
        "                if pdf_url in processed_urls:\n",
        "                    continue\n",
        "\n",
        "                link_text = link.get_text(strip=True).lower()\n",
        "\n",
        "                is_pdf_link = (\n",
        "                    href.lower().endswith('.pdf') or\n",
        "                    'download' in link_text or\n",
        "                    'view pdf' in link_text\n",
        "                )\n",
        "\n",
        "                if is_pdf_link:\n",
        "                    processed_urls.add(pdf_url)\n",
        "                    print(f\"  - Found potential PDF link: {pdf_url}\")\n",
        "                    try:\n",
        "                        pdf_response = requests.get(pdf_url, headers=headers, timeout=10)\n",
        "                        pdf_response.raise_for_status()\n",
        "                        if 'application/pdf' in pdf_response.headers.get('Content-Type', ''):\n",
        "                            extracted_text = extract_text_from_pdf(pdf_response.content)\n",
        "                            if extracted_text:\n",
        "                                print(\"  - Successfully extracted text from PDF.\")\n",
        "                                full_text = extracted_text\n",
        "                                break\n",
        "                        else:\n",
        "                            print(f\"  - Link did not lead to a PDF. Content-Type: {pdf_response.headers.get('Content-Type')}\")\n",
        "\n",
        "                    except requests.exceptions.RequestException as e:\n",
        "                        print(f\"  - Failed to download PDF from {pdf_url}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        # Method 4: Final fallback to scraping HTML body text\n",
        "        if full_text == \"Full text not found\":\n",
        "            print(\"  - No usable PDF found. Falling back to scraping HTML body.\")\n",
        "            content_area = soup.find('article') or soup.find('div', id='content') or soup.find('div', class_='content') or soup.find('div', role='main')\n",
        "            if content_area:\n",
        "                full_text = content_area.get_text(separator='\\n', strip=True)\n",
        "            else:\n",
        "                body_tag = soup.find('body')\n",
        "                if body_tag:\n",
        "                    full_text = body_tag.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'abstract': abstract,\n",
        "            'full_text': full_text,\n",
        "            'url': url\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while parsing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_links_from_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads a file of URLs, scrapes each one, and returns the data.\n",
        "\n",
        "    Args:\n",
        "        filepath: The path to the text or csv file with URLs.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary is a scraped paper.\n",
        "    \"\"\"\n",
        "    scraped_data = []\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            is_csv = '.csv' in filepath.lower()\n",
        "            if is_csv:\n",
        "                reader = csv.reader(f)\n",
        "                urls = [row[0] for row in reader if row]\n",
        "            else:\n",
        "                urls = f.read().splitlines()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "        return []\n",
        "\n",
        "    for url in urls:\n",
        "        if url.strip():\n",
        "            print(f\"Scraping: {url}\")\n",
        "            data = scrape_paper(url)\n",
        "            if data:\n",
        "                scraped_data.append(data)\n",
        "    return scraped_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"paper_links.txt\", \"w\") as f:\n",
        "        f.write(\"https://arxiv.org/abs/1706.03762\\n\") # Attention is All You Need\n",
        "        f.write(\"https://www.sciencedirect.com/science/article/pii/S2213846323001116\\n\") # ScienceDirect Paper\n",
        "        f.write(\"https://invalid-url-example.com\\n\") # Example of a failed URL\n",
        "\n",
        "    papers = process_links_from_file('paper_links.txt')\n",
        "\n",
        "    if papers:\n",
        "        output_filename = 'scraped_papers.csv'\n",
        "        with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = ['title', 'authors', 'abstract', 'full_text', 'url']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for paper in papers:\n",
        "                writer.writerow(paper)\n",
        "\n",
        "        print(f\"\\nScraping complete. Data saved to {output_filename}\")\n",
        "        print(f\"Successfully scraped {len(papers)} papers.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq1Fnt0KAuAi",
        "outputId": "c47834a2-92a5-4d1b-a9d1-8b37010d8c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://arxiv.org/abs/1706.03762\n",
            "  - Selenium error: Message: \n",
            "Stacktrace:\n",
            "#0 0x5a0745f7794a <unknown>\n",
            "#1 0x5a07459ec8a0 <unknown>\n",
            "#2 0x5a0745a3e540 <unknown>\n",
            "#3 0x5a0745a3e731 <unknown>\n",
            "#4 0x5a0745a8c824 <unknown>\n",
            "#5 0x5a0745a6405d <unknown>\n",
            "#6 0x5a0745a89c23 <unknown>\n",
            "#7 0x5a0745a63e03 <unknown>\n",
            "#8 0x5a0745a30968 <unknown>\n",
            "#9 0x5a0745a315e1 <unknown>\n",
            "#10 0x5a0745f3b548 <unknown>\n",
            "#11 0x5a0745f3f272 <unknown>\n",
            "#12 0x5a0745f22313 <unknown>\n",
            "#13 0x5a0745f3fdc5 <unknown>\n",
            "#14 0x5a0745f0749f <unknown>\n",
            "#15 0x5a0745f64158 <unknown>\n",
            "#16 0x5a0745f64332 <unknown>\n",
            "#17 0x5a0745f76a53 <unknown>\n",
            "#18 0x7980dd67eac3 <unknown>\n",
            "\n",
            "Error: Could not retrieve page source for https://arxiv.org/abs/1706.03762\n",
            "Scraping: https://www.sciencedirect.com/science/article/pii/S2213846323001116\n",
            "  - Selenium error: Message: \n",
            "Stacktrace:\n",
            "#0 0x58c3e675994a <unknown>\n",
            "#1 0x58c3e61ce8a0 <unknown>\n",
            "#2 0x58c3e6220540 <unknown>\n",
            "#3 0x58c3e6220731 <unknown>\n",
            "#4 0x58c3e626e824 <unknown>\n",
            "#5 0x58c3e624605d <unknown>\n",
            "#6 0x58c3e626bc23 <unknown>\n",
            "#7 0x58c3e6245e03 <unknown>\n",
            "#8 0x58c3e6212968 <unknown>\n",
            "#9 0x58c3e62135e1 <unknown>\n",
            "#10 0x58c3e671d548 <unknown>\n",
            "#11 0x58c3e6721272 <unknown>\n",
            "#12 0x58c3e6704313 <unknown>\n",
            "#13 0x58c3e6721dc5 <unknown>\n",
            "#14 0x58c3e66e949f <unknown>\n",
            "#15 0x58c3e6746158 <unknown>\n",
            "#16 0x58c3e6746332 <unknown>\n",
            "#17 0x58c3e6758a53 <unknown>\n",
            "#18 0x7f350ccbaac3 <unknown>\n",
            "\n",
            "Error: Could not retrieve page source for https://www.sciencedirect.com/science/article/pii/S2213846323001116\n",
            "Scraping: https://invalid-url-example.com\n",
            "  - Selenium error: Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
            "  (Session info: chrome=140.0.7339.80)\n",
            "Stacktrace:\n",
            "#0 0x55f3083dd94a <unknown>\n",
            "#1 0x55f307e528a0 <unknown>\n",
            "#2 0x55f307e49ce8 <unknown>\n",
            "#3 0x55f307e3b1b6 <unknown>\n",
            "#4 0x55f307e3cf00 <unknown>\n",
            "#5 0x55f307e3b55e <unknown>\n",
            "#6 0x55f307e3aee7 <unknown>\n",
            "#7 0x55f307e3abc5 <unknown>\n",
            "#8 0x55f307e38a42 <unknown>\n",
            "#9 0x55f307e3922a <unknown>\n",
            "#10 0x55f307e55db9 <unknown>\n",
            "#11 0x55f307ef0835 <unknown>\n",
            "#12 0x55f307eca032 <unknown>\n",
            "#13 0x55f307eefc23 <unknown>\n",
            "#14 0x55f307ec9e03 <unknown>\n",
            "#15 0x55f307e96968 <unknown>\n",
            "#16 0x55f307e975e1 <unknown>\n",
            "#17 0x55f3083a1548 <unknown>\n",
            "#18 0x55f3083a5272 <unknown>\n",
            "#19 0x55f308388313 <unknown>\n",
            "#20 0x55f3083a5dc5 <unknown>\n",
            "#21 0x55f30836d49f <unknown>\n",
            "#22 0x55f3083ca158 <unknown>\n",
            "#23 0x55f3083ca332 <unknown>\n",
            "#24 0x55f3083dca53 <unknown>\n",
            "#25 0x7f8732470ac3 <unknown>\n",
            "\n",
            "Error: Could not retrieve page source for https://invalid-url-example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "CRY7aHwDYeC2",
        "outputId": "cddd4cff-8bd1-48a0-b458-2426f719a82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,005 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,789 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,239 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.0 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,273 kB]\n",
            "Fetched 24.3 MB in 3s (8,635 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 32.5 MB of archives.\n",
            "After this operation, 130 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.68.5+ubuntu22.04.1 [30.0 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 32.5 MB in 3s (10.4 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126574 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.68.5+ubuntu22.04.1_amd64.deb ...\n",
            "Unpacking snapd (2.68.5+ubuntu22.04.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service  /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.68.5+ubuntu22.04.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service  /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service  /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service  /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service  /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service  /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service  /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service  /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer  /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket  /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service  /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126801 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              },
              "id": "ab69d48540d048118538393f56113abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}